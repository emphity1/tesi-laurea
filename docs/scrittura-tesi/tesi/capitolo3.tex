% !TeX spellcheck = it_IT
\chapter{Architettura MobileNetECA}
\label{chap:mobileneteca}

In questo capitolo analizziamo l'architettura proposta, partendo dal design complessivo e approfondendo le componenti chiave: il blocco inverso residuo modificato, il meccanismo di attenzione ECA e la tecnica di Structural Reparameterization.

\section{Design Complessivo dell'Architettura}

MobileNetECA è un'architettura convoluzionale compatta progettata specificatamente per CIFAR-10 (immagini 32×32). Combina l'efficienza di MobileNetV2 con il meccanismo di attenzione leggero ECA e sfrutta la ri-parametrizzazione strutturale per migliorare le prestazioni.

\section{Structural Reparameterization}
Una delle innovazioni chiave adottate in questo lavoro per spingere l'accuratezza oltre il 92\% senza aumentare il costo computazionale all'inferenza è la \textbf{Structural Reparameterization}, ispirata da RepVGG~\cite{ding2021repvgg}.

\subsection{Concetto Chiave: Training vs Inference}
L'idea fondamentale è disaccoppiare l'architettura usata durante il training da quella usata per l'inferenza (deploy).
\begin{itemize}
    \item \textbf{Training (Multi-Branch)}: Ogni blocco convoluzionale è arricchito con rami paralleli (es. 1x1 conv, identity mapping). Questo facilita il flusso dei gradienti, agendo come un ensemble implicito e facilitando l'ottimizzazione (niente "dead zones").
    \item \textbf{Inference (Single-Branch)}: Prima del deploy, i rami paralleli vengono matematicamente "fusi" in un unico kernel 3x3. Il risultato è una rete piana (VGG-style) o un blocco MobileNet standard, estremamente veloce.
\end{itemize}

\subsection{Fusione Matematica dei Kernel}
Consideriamo un input $X$ e tre rami paralleli che computano l'output $Y$:
\begin{enumerate}
    \item Conv 3x3 ($W^{(3)}$, biases $\mu^{(3)}, \sigma^{(3)}, \gamma^{(3)}, \beta^{(3)}$ di BN)
    \item Conv 1x1 ($W^{(1)}$, biases $\mu^{(1)}, \dots$)
    \item Identity (se dimensioni matchano)
\end{enumerate}

Poiché la convoluzione è un operatore lineare, possiamo sommare i kernel.
Per prima cosa, trasformiamo ogni conv+BN in un singolo kernel con bias:
\begin{equation}
W'_{i} = W_i \cdot \frac{\gamma_i}{\sigma_i}, \quad b'_i = \beta_i - \mu_i \frac{\gamma_i}{\sigma_i}
\end{equation}

Successivamente, facciamo il padding del kernel 1x1 (diventa 3x3 con zeri attorno) e dell'identità (matrice identità al centro, zeri altrove).
Il kernel finale $W_{final}$ sarà semplicemente:
\begin{equation}
W_{final} = W'^{(3)} + \text{pad}(W'^{(1)}) + \text{weight}(\text{identity})
\end{equation}

Otteniamo così un singolo strato convoluzionale $Y = W_{final} * X + b_{final}$ matematicamente equivalente alla somma dei tre rami, ma molto più efficiente.

\subsection{Perché l'abbiamo usata?}
La baseline MobileNetV2 standard faticava a superare il 91.5\% a causa della sua profondità e "sottigliezza" (pochi canali). L'aggiunta dei rami di re-parametrizzazione ha permesso di addestrare una rete virtualmente più capace, catturando feature più ricche, per poi "collassarla" nella forma compatta originale. Questo ci ha regalato circa uno \textbf{0.5-0.8\% di accuratezza "gratuita"} (costo zero all'inferenza).

\subsection{Pipeline Architetturale}
L'architettura è composta da 4 stage principali:

\begin{enumerate}
    \item \textbf{Stem}: Conv 3×3, stride=1, BN, GELU (32×32 → 32×32)
    \item \textbf{Stage 1-4}: Inverted Residual Blocks con ECA
    \item \textbf{Head}: Conv 1×1 + Global Average Pooling
    \item \textbf{Classifier}: Linear layer (10 classi)
\end{enumerate}

L'architettura completa è composta da:
\begin{enumerate}
    \item Un layer convoluzionale iniziale (Conv2d 3x3).
    \item 12 blocchi Inverted Residual che incrementano progressivamente il numero di canali.
    \item I blocchi ECA vengono inseriti dopo le espansioni, per calibrare le feature map intermedie.
    \item Un layer di classificazione finale (Linear) dopo un Global Average Pooling.
\end{enumerate}

\subsection{Tabella di Configurazione}
La Tabella~\ref{tab:block_config} descrive la configurazione dei blocchi.

\begin{table}[h]
\centering
\caption{Configurazione blocchi MobileNetECA}
\label{tab:block_config}
\begin{tabular}{cccccc}
\toprule
\textbf{Blocco} & \textbf{$t$ (expand)} & \textbf{$c$ (out)} & \textbf{$n$ (repeat)} & \textbf{$s$ (stride)} & \textbf{use\_eca} \\
\midrule
1 & 1 & 20 & 2 & 1 & True \\
2 & 6 & 32 & 4 & 2 & True \\
3 & 8 & 42 & 4 & 2 & True \\
4 & 8 & 52 & 2 & 1 & True \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Legenda}:
\begin{itemize}
    \item $t$: Expansion ratio (moltiplicatore per i canali interni)
    \item $c$: Output channels (prima di width multiplier)
    \item $n$: Numero di ripetizioni del blocco
    \item $s$: Stride (applicato solo al primo blocco della ripetizione)
    \item use\_eca: Se applicare ECA dopo depthwise conv
\end{itemize}

\subsection{Width Multiplier}
Il parametro $\alpha$ (width multiplier) scala uniformemente tutti i canali:
\begin{equation}
C_{\text{effective}} = \max\left(\lfloor \alpha \cdot C_{\text{base}}\rfloor, 8\right)
\end{equation}

Valori testati: $\alpha \in \{0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6\}$

\textbf{Trade-off}:
\begin{itemize}
    \item $\alpha$ piccolo: Modello più leggero ma meno capacità
    \item $\alpha$ grande: Più accuratezza ma più parametri/FLOPs
\end{itemize}

\section{Inverted Residual Block con ECA}

\subsection{Struttura Dettagliata}
\begin{figure}[h]
\centering
\begin{verbatim}
Input (C_in channels, H×W spatial)
    ↓
[se expand_ratio > 1]
    Conv 1×1: C_in → hidden_dim
    BatchNorm
    GELU
    ↓
Depthwise Conv 3×3, stride=s
    (groups = hidden_dim)
    BatchNorm
    GELU
    ↓
ECA Block (attention ricalibration)
    GlobalAvgPool
    Conv1D (kernel=k_adaptive)
    Sigmoid
    Multiply
    ↓
Projection Conv 1×1: hidden_dim → C_out
    BatchNorm (linear, no activation!)
    ↓
[se stride=1 AND C_in=C_out]
    + Skip Connection (Residual)
    ↓
Output (C_out channels, H/s × W/s spatial)
\end{verbatim}
\caption{Pipeline Inverted Residual Block con ECA}
\label{fig:inv_res_eca}
\end{figure}

\subsection{Implementazione Codice}
Estratto semplificato da \texttt{model.py}:

\begin{lstlisting}[language=Python, caption=InvertedResidual con ECA, basicstyle=\footnotesize\ttfamily, lineskip=-1pt]
class InvertedResidual(nn.Module):
    def __init__(self, inp, oup, stride, expand_ratio, use_eca=False):
        super().__init__()
        hidden_dim = int(inp * expand_ratio)
        self.use_res_connect = (stride == 1 and inp == oup)
        layers = []
        
        # Expansion
        if expand_ratio != 1:
            layers.extend([
                nn.Conv2d(inp, hidden_dim, 1, bias=False),
                nn.BatchNorm2d(hidden_dim),
                nn.GELU()
            ])
        
        # Depthwise
        layers.extend([
            nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),
            nn.BatchNorm2d(hidden_dim),
            nn.GELU()
        ])
        
        # ECA Attention
        if use_eca: layers.append(ECABlock(hidden_dim))
        
        # Projection
        layers.extend([
            nn.Conv2d(hidden_dim, oup, 1, bias=False),
            nn.BatchNorm2d(oup)
        ])
        
        self.conv = nn.Sequential(*layers)

    def forward(self, x):
        if self.use_res_connect: return x + self.conv(x)
        return self.conv(x)
\end{lstlisting}

\section{ECA Block: Implementazione Dettagliata}

\subsection{Formula del Kernel Adattivo}
Il kernel size della Conv1D è calcolato dinamicamente:

\begin{equation}
t = \left|\frac{\log_2(C) + b}{\gamma}\right|
\end{equation}

\begin{equation}
k = \begin{cases}
t & \text{se } t \text{ è dispari} \\
t + 1 & \text{altrimenti}
\end{cases}
\end{equation}

Con parametri default: $b=12$, $\gamma=3$.

\textbf{Motivazione}: Più canali → kernel più grande per catturare dipendenze a lungo raggio.

\textbf{Esempi}:
\begin{itemize}
    \item $C=32$: $t = |(5 + 12)/3| = 5.67 \to k=5$
    \item $C=64$: $t = |(6 + 12)/3| = 6 \to k=7$
    \item $C=144$: $t = |(7.17 + 12)/3| = 6.39 \to k=7$
\end{itemize}

\subsection{Forward Pass Step-by-Step}
\begin{enumerate}
    \item \textbf{Input}: $\mathbf{x} \in \mathbb{R}^{B \times C \times H \times W}$
    
    \item \textbf{Global Average Pooling}:
    \begin{equation}
    \mathbf{y} = \text{GAP}(\mathbf{x}) = \frac{1}{H \cdot W} \sum_{i=1}^{H}\sum_{j=1}^{W} \mathbf{x}[:,:,i,j]
    \end{equation}
    Output: $\mathbf{y} \in \mathbb{R}^{B \times C \times 1 \times 1}$
    
    \item \textbf{Reshape}: $\mathbf{y} \in \mathbb{R}^{B \times 1 \times C}$ (per Conv1D)
    
    \item \textbf{Conv1D}: Kernel size $k$, padding $(k-1)/2$
    \begin{equation}
    \mathbf{y} = \text{Conv1D}_k(\mathbf{y})
    \end{equation}
    Output: $\mathbf{y} \in \mathbb{R}^{B \times 1 \times C}$
    
    \item \textbf{Sigmoid activation}:
    \begin{equation}
    \mathbf{y} = \sigma(\mathbf{y}) = \frac{1}{1 + e^{-\mathbf{y}}}
    \end{equation}
    Weights $\in [0, 1]$
    
    \item \textbf{Reshape back}: $\mathbf{y} \in \mathbb{R}^{B \times C \times 1 \times 1}$
    
    \item \textbf{Gradient Scaling}:
    \begin{equation}
    \mathbf{y} = \mathbf{y} \cdot \alpha + \mathbf{y}_{\text{stop\_grad}} \cdot (1 - \alpha)
    \end{equation}
    Con $\alpha = \text{lr\_scale} = 1.6$
    
    \item \textbf{Element-wise Multiply}:
    \begin{equation}
    \mathbf{x}_{\text{out}} = \mathbf{x} \odot \mathbf{y}
    \end{equation}
    Output: $\mathbf{x}_{\text{out}} \in \mathbb{R}^{B \times C \times H \times W}$
\end{enumerate}

\subsection{Codice Implementazione}
\begin{lstlisting}[language=Python, caption=ECA Block, basicstyle=\footnotesize\ttfamily, lineskip=-1pt]
class ECABlock(nn.Module):
    def __init__(self, channels, gamma=3, b=12):
        super().__init__()
        # Adaptive kernel size
        t = int(abs((math.log(channels, 2) + b) / gamma))
        k = t if t % 2 else t + 1
        
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.conv = nn.Conv1d(1, 1, kernel_size=k, padding=(k - 1) // 2, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        # Squeeze [B, C, H, W] -> [B, C, 1, 1]
        y = self.avg_pool(x)
        
        # Excitation (con Conv1D efficiente)
        y = self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)
        y = self.sigmoid(y)
        
        # Scale
        return x * y.expand_as(x)
\end{lstlisting}

\section{Analisi della Complessità}

\subsection{Parametri}
Per width\_mult = 0.5 (configurazione finale):
\begin{itemize}
    \item \textbf{Parametri Training}: \textasciitilde 84,700 (dovuti ai branch paralleli)
    \item \textbf{Parametri Inference}: \textasciitilde 76,600 (dopo re-parameterization)
    \item Stem: \textasciitilde 1.5k
    \item Inverted Residual Blocks: \textasciitilde 70k
    \item ECA Blocks: \textasciitilde 3k
    \item Classifier: \textasciitilde 1.5k
\end{itemize}

\subsection{FLOPs}
\textbf{Multiply-Accumulate Operations (MACs)}: \textasciitilde 10-12M

Breakdown approssimativo:
\begin{itemize}
    \item Depthwise convs: \textasciitilde 60\%
    \item Pointwise convs: \textasciitilde 35\%
    \item ECA blocks: <1\% (trascurabile)
\end{itemize}

\subsection{Confronto con Altri Modelli}
\begin{table}[h]
\centering
\caption{Confronto complessità computazionale (Inference)}
\label{tab:complexity}
\begin{tabular}{lccc}
\toprule
\textbf{Modello} & \textbf{Params} & \textbf{MACs} & \textbf{Size (KB)} \\
\midrule
\textbf{MobileNetECA (ours)} & \textbf{76.6k} & \textbf{\textasciitilde 11M} & \textbf{\textasciitilde 300} \\
MobileNetV2 $\times$0.5 & 700k & 28M & 2,800 \\
MobileNetV2 $\times$1.0 & 2.24M & 88M & 8,960 \\
ResNet-20 & 270k & 41M & 1,080 \\
ShuffleNetV2 $\times$0.5 & 350k & 10M & 1,400 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Osservazioni}:
\begin{itemize}
    \item 13× meno parametri di MobileNetV2 ×0.5
    \item 4× meno FLOPs di MobileNetV2 ×0.5
    \item Dimensione modello < 220 KB (fit in L2 cache!)
\end{itemize}

\section{Scelte Implementative}

\subsection{Optimizer}
\textbf{SGD con Momentum}:
\begin{equation}
v_t = \beta v_{t-1} + \nabla_\theta L(\theta_{t-1})
\end{equation}
\begin{equation}
\theta_t = \theta_{t-1} - \eta v_t
\end{equation}

Parametri: $\beta = 0.9$ (momentum), $\eta$ variabile (learning rate).

\textbf{Perché SGD vs Adam?}
\begin{itemize}
    \item SGD + momentum generalizza meglio
    \item Adam converge più velocemente ma può overfittare
    \item Per modelli compatti, generalizzazione > velocità
\end{itemize}

\subsection{Learning Rate Schedule}
\textbf{Cosine Annealing}:
\begin{equation}
\eta_t = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})\left(1 + \cos\left(\frac{t \pi}{T}\right)\right)
\end{equation}

dove $t$ è l'epoca corrente, $T$ è il totale epoche.

\textbf{Vantaggi}:
\begin{itemize}
    \item Decay smooth (vs step decay)
    \item Esplorazione iniziale alta, fine-tuning finale basso
    \item No hyperparameter (solo $T$)
\end{itemize}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figure/lr_schedule_200.png}
    \caption{Learning Rate Schedule: Cosine Annealing. Il tasso di apprendimento decresce da 0.05 a 0 in modo graduale.}
    \label{fig:lr_schedule}
\end{figure}

\subsection{Weight Decay}
Regolarizzazione L2:
\begin{equation}
L_{\text{total}} = L_{\text{CE}} + \lambda \sum_{i} w_i^2
\end{equation}

Valori testati: $\lambda \in \{10^{-4}, 2 \times 10^{-4}, 3 \times 10^{-4}, 5 \times 10^{-4}, 10^{-3}\}$

\subsection{Data Augmentation}
\textbf{Training}:
\begin{itemize}
    \item RandomCrop(32, padding=4): Crop casuale con padding
    \item RandomHorizontalFlip(p=0.5): Flip orizzontale
    \item Normalize: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.243, 0.261)
\end{itemize}

\textbf{Test}: SOLO Normalize (NO augmentation!)



\subsection{Gradient Clipping}
\begin{equation}
\mathbf{g}_{\text{clipped}} = \min\left(1, \frac{\text{max\_norm}}{\|\mathbf{g}\|_2}\right) \mathbf{g}
\end{equation}

Con max\_norm = 5.0.

\textbf{Previene}: Esplosione dei gradienti durante training.

\textbf{[AGGIUNGERE DIAGRAMMA TRAINING PIPELINE]}
