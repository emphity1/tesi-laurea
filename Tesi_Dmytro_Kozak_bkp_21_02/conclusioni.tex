% !TeX spellcheck = it_IT

Questo lavoro ha presentato lo sviluppo e l'ottimizzazione di \textbf{MobileNetECA-Rep-AdvAug}, un'architettura neurale ultra-compatta per la classificazione di immagini su CIFAR-10. L'obiettivo principale era esplorare il trade-off tra efficienza computazionale e accuratezza, dimostrando che modelli estremamente leggeri possono raggiungere prestazioni competitive attraverso l'integrazione di meccanismi di attenzione moderni e un'ottimizzazione sistematica degli iperparametri.

\section*{Sintesi del Lavoro Svolto}

Il progetto si è articolato in quattro fasi principali:

\begin{enumerate}
    \item \textbf{Progettazione architetturale}: Implementazione di MobileNetECA combinando blocchi Inverted Residual (da MobileNetV2) con meccanismi Efficient Channel Attention (ECA), utilizzando GELU come funzione di attivazione per maggiore stabilità.
    
    \item \textbf{Structural Reparameterization}: L'adozione di queste tecniche di ri-parametrizzazione strutturale ha permesso di addestrare una rete più capace e complessa (multi-branch) che collassa in un modello efficiente all'inferenza.
    
    \item \textbf{Grid search sistematica}: Esplorazione di centinaia di configurazioni di iperparametri attraverso un approccio two-phase (screening rapido + refinement), identificando empiricamente la combinazione ottimale di learning rate, width multiplier e weight decay.
    
    \item \textbf{Validazione sperimentale}: Allenamento del modello ottimizzato per 200 epoche con SGD+momentum e Cosine Annealing, raggiungendo un'accuratezza finale di \textbf{93.76\%} sul test set CIFAR-10, superando le baseline standard.
\end{enumerate}

\section*{Risultati Principali}

Mo\-bile\-Net\-ECA-Rep-AdvAug ha dimostrato un eccellente trade-off efficienza-accuratezza:

\begin{itemize}
    \item \textbf{Accuratezza}: 93.76\% su CIFAR-10 test set.
    \item \textbf{Parametri}: 62,194 (ridotto del 20\% rispetto alla Baseline interna, e circa 11 volte meno di una MobileNetV2 standard $\times$0.5 da letteratura).
    \item \textbf{FLOPs}: $\sim$9.4M MACs (quasi 3 volte meno di MobileNetV2 standard $\times$0.5).
    \item \textbf{Dimensione Modello}: Meno di 250 KB (fit comodo in L2 cache).
    \item \textbf{Contributo ECA}: +0.52\% accuracy con overhead minimo.
\end{itemize}

Il modello si posiziona competitivamente nella frontiera ultra-compatta ($<$ 100k parametri), risultando ideale per scenari con vincoli estremi di memoria, latenza ed energia.

\section*{Contributi della Tesi}

\textbf{1. Implementazione}:
\begin{itemize}
    \item Versione ottimizzata di MobileNetECA specifica per CIFAR-10 (immagini a bassa risoluzione $32 \times 32$).
    \item Integrazione seamless di ECA blocks in architettura Inverted Residual.
    \item Implementazione efficiente della ri-parametrizzazione per il deploy.
\end{itemize}

\textbf{2. Metodologia}:
\begin{itemize}
    \item Grid search sistematica con visualizzazione tramite heatmap per comprendere le interazioni tra iperparametri.
    \item Protocollo di valutazione rigoroso e riproducibile.
\end{itemize}

\textbf{3. Validazione}:
\begin{itemize}
    \item Quantificazione empirica del contributo ECA tramite ablation study.
    \item Analisi dettagliata degli errori tramite Matrice di Confusione e Curve ROC.
\end{itemize}

\section*{Sviluppi Futuri}

\subsubsection*{Architettura}
\begin{itemize}
    \item \textbf{Altri meccanismi di attenzione}: Sostituire l'ECA con moduli CBAM (Convolutional Block Attention Module) o Coordinate Attention. Mentre l'ECA ottimizza solo i canali, l'integrazione dell'attenzione spaziale potrebbe mitigare l'errore di \textit{background confusion} osservato (Section 6.5), con uno stimato guadagno di $+0.2\% - 0.4\%$ in accuratezza, al costo di un lieve incremento parametrale ($\approx+2k$ parametri).
    \item \textbf{Neural Architecture Search (NAS)}: Adottare algoritmi NAS per parametrizzare singolarmente ogni livello della rete (es. \textit{expansion ratio} eterogenei tra $3\times$ e $6\times$, o kernel misti $3\times3$ e $5\times5$). Un approccio hardware-aware NAS potrebbe plausibilmente ridurre i parametri totali di un ulteriore $10 - 15\%$ mantenendo costante l'accuratezza attuale.
    \item \textbf{Hybrid CNN-Transformer}: Esplorare l'integrazione di strati di \textit{MobileViT} o \textit{self-attention} nelle fasi terminali della rete. Questo approccio ibrido migliorerebbe la cattura delle dipendenze a lungo raggio, particolarmente utile se si scala il modello per dataset visivi a risoluzione maggiore ($224 \times 224$).
\end{itemize}

\subsubsection*{Ottimizzazione e Compressione}
\begin{itemize}
    \item \textbf{Quantization (INT8)}: Convertire i pesi e le attivazioni del modello dal formato floating-point (FP32) a interi a 8-bit (INT8) tramite \textit{Post-Training Quantization} o \textit{Quantization-Aware Training}. Questa operazione ridurrebbe il memory footprint da $\approx 250$ KB a soli $\approx 65$ KB e garantirebbe uno \textit{speedup} latenziale stimato tra $2\times$ e $4\times$ su microcontrollori dotati di istruzioni SIMD vettoriali (es. architettura ARM Cortex-M con estensioni DSP), con un degrado dell'accuratezza solitamente trascurabile (inferiore allo $0.5\%$).
    \item \textbf{Pruning Strutturato}: Applicare algoritmi di \textit{channel pruning} post-addestramento (o basati sulla regolarizzazione L1 nel training) per recidere i filtri prossimi allo zero. Dato l'ampio uso di \textit{expansion layer} ($6\times$) nel nostro modello, si stima che un pruning conservativo possa recidere fino al $20\%$ delle FLOPs (scendendo sotto i $7.5$ MFLOPs) senza compromettere la stabilità del modello distillato.
\end{itemize}

\subsubsection*{Deploy ed Edge Computing}
\begin{itemize}
    \item \textbf{Conversione framework}: Export del modello in formati ottimizzati come TensorFlow Lite per Microcontrollori o ONNX Runtime.
    \item \textbf{Testing hardware reale}: Validazione su dispositivi fisici come Raspberry Pi Pico, ESP32 o NVIDIA Jetson Nano per misurare consumi energetici e latenza reale.
\end{itemize}

\section*{Impatto Pratico e Riflessioni Finali}

Modelli ultra-compatti come MobileNetECA abilitano applicazioni AI precedentemente impraticabili su dispositivi con risorse limitate.

\textbf{Privacy e Sicurezza}: L'inferenza interamente on-device preserva la privacy degli utenti, poiché i dati grezzi (immagini) non devono mai lasciare il dispositivo per essere processati nel cloud. Questo riduce anche la dipendenza dalla connettività di rete, garantendo il funzionamento in aree remote o offline.

\textbf{Sostenibilità Ambientale}: Ridurre il costo computazionale dell'inferenza significa ridurre direttamente il consumo energetico. In un mondo con miliardi di dispositivi IoT, l'efficienza algoritmica ha un impatto tangibile sulla carbon footprint globale del settore tecnologico.

\textbf{Democratizzazione dell'AI}: L'accessibilità di modelli performanti su hardware a basso costo permette di portare soluzioni intelligenti in settori e regioni geografiche dove l'investimento in costose infrastrutture server non è sostenibile.

In conclusione, questo lavoro dimostra che l'efficienza e l'accuratezza non sono obiettivi mutuamente esclusivi. Attraverso un design architetturale oculato, l'uso di meccanismi di attenzione leggeri e un'ottimizzazione sistematica, è possibile sviluppare modelli che bilanciano prestazioni competitive con requisiti computazionali minimi. Il futuro dell'AI non risiede solo in modelli sempre più grandi, ma anche in soluzioni efficienti e sostenibili capaci di operare ovunque.
