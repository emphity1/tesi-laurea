\relax 
\providecommand*{\memsetcounter}[2]{}
\providecommand \babel@aux [2]{\global \let \babel@toc \@gobbletwo }
\@nameuse{bbl@beforestart}
\bibstyle{ieeetr}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\babel@aux{italian}{}
\babel@aux{italian}{}
\@writefile{toc}{\contentsline {chapter}{Indice}{iii}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Elenco delle figure}{vii}{section*.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {1}Introduzione}{1}{chapter.1}\protected@file@percent }
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{cap:capitolo1}{{1}{1}{Introduzione}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Contesto e Motivazione}{1}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Obiettivi della Tesi}{2}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Contributi del Lavoro}{2}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Struttura della Tesi}{3}{section.1.4}\protected@file@percent }
\citation{krizhevsky2009learning}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {2}Il Dataset CIFAR-10}{5}{chapter.2}\protected@file@percent }
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{cap:capitolo2}{{2}{5}{Il Dataset CIFAR-10}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Descrizione del Dataset CIFAR-10}{5}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Struttura e Organizzazione dei Dati}{6}{subsection.2.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Esempi di Immagini dal Dataset CIFAR-10. La bassa risoluzione ($32 \times 32$) rende il compito di classificazione sfidante anche per l'occhio umano, specialmente per classi visivamente simili.}}{7}{figure.2.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:cifar10_sample}{{2.1}{7}{Esempi di Immagini dal Dataset CIFAR-10. La bassa risoluzione ($32 \times 32$) rende il compito di classificazione sfidante anche per l'occhio umano, specialmente per classi visivamente simili}{figure.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Preprocessing dei Dati}{7}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Normalizzazione}{7}{subsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Data Augmentation Standard}{8}{subsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Sfide e Limitazioni}{8}{section.2.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {3}Fondamenti delle Reti Neurali}{10}{chapter.3}\protected@file@percent }
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{cap:capitolo3}{{3}{10}{Fondamenti delle Reti Neurali}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Apprendimento Automatico}{10}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Paradigmi di Apprendimento}{10}{subsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Il Perceptron e il Neurone Artificiale}{11}{subsection.3.1.2}\protected@file@percent }
\citation{hendrycks2016gaussian}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Funzioni di Attivazione}{12}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Sigmoide e Tanh}{12}{subsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}ReLU (Rectified Linear Unit)}{12}{subsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}GELU (Gaussian Error Linear Unit)}{13}{subsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Addestramento della Rete}{13}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Funzione di Costo (Loss Function)}{13}{subsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Algoritmo di Backpropagation}{13}{subsection.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Ottimizzazione e Regolarizzazione}{14}{section.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Ottimizzatori}{14}{subsection.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1.1}Stochastic Gradient Descent (SGD)}{14}{subsubsection.3.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1.2}SGD con Momentum}{14}{subsubsection.3.4.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1.3}Adam (Adaptive Moment Estimation)}{15}{subsubsection.3.4.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Tecniche di Regolarizzazione}{15}{subsection.3.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2.1}Batch Normalization}{15}{subsubsection.3.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2.2}Dropout}{15}{subsubsection.3.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2.3}Weight Decay (L2 Regularization)}{15}{subsubsection.3.4.2.3}\protected@file@percent }
\citation{he2016deep}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {4}Stato dell'Arte e Tecniche di Efficienza}{16}{chapter.4}\protected@file@percent }
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{cap:capitolo4}{{4}{16}{Stato dell'Arte e Tecniche di Efficienza}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Evoluzione delle Architetture CNN}{16}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Dalle Origini al Deep Learning Moderno}{16}{subsection.4.1.1}\protected@file@percent }
\citation{howard2017mobilenets}
\citation{sandler2018mobilenetv2}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Architetture Efficienti per Mobile}{17}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Depthwise Separable Convolutions}{17}{subsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}MobileNetV2: Inverted Residuals}{17}{subsection.4.2.2}\protected@file@percent }
\citation{hu2018squeeze}
\citation{wang2020eca}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Meccanismi di Attenzione}{18}{section.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Squeeze-and-Excitation (SE)}{18}{subsection.4.3.1}\protected@file@percent }
\citation{ding2021repvgg}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Efficient Channel Attention (ECA)}{19}{subsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Structural Reparameterization (RepVGG)}{19}{section.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Il Principio di Equivalenza}{19}{subsection.4.4.1}\protected@file@percent }
\citation{ding2021repvgg}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {5}Progettazione e Architettura del Sistema}{20}{chapter.5}\protected@file@percent }
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{cap:capitolo5}{{5}{20}{Progettazione e Architettura del Sistema}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Panoramica del Sistema}{20}{section.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Architettura MobileNetECARep. I blocchi azzurri rappresentano i tensori intermedi. Il dettaglio mostra il modulo RepInvertedResidual con integrazione di Efficient Channel Attention (ECA) e la struttura di convoluzione riparametrizzabile (RepConv).}}{21}{figure.5.1}\protected@file@percent }
\newlabel{fig:mobilenet_eca_rep_overview}{{5.1}{21}{Architettura MobileNetECARep. I blocchi azzurri rappresentano i tensori intermedi. Il dettaglio mostra il modulo RepInvertedResidual con integrazione di Efficient Channel Attention (ECA) e la struttura di convoluzione riparametrizzabile (RepConv)}{figure.5.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Structural Reparameterization}{21}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Concetto Chiave: Training vs Inference}{21}{subsection.5.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Fusione Matematica dei Kernel: Derivazione Completa}{22}{subsection.5.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Visualizzazione della fusione dei kernel. Il kernel $1 \times 1$ viene portato a $3 \times 3$ tramite zero-padding, mentre l'identità diventa un kernel $3 \times 3$ con 1 al centro e 0 altrove. La somma finale produce un unico kernel $3 \times 3$ equivalente.}}{23}{figure.5.2}\protected@file@percent }
\newlabel{fig:kernel_fusion_viz}{{5.2}{23}{Visualizzazione della fusione dei kernel. Il kernel $1 \times 1$ viene portato a $3 \times 3$ tramite zero-padding, mentre l'identità diventa un kernel $3 \times 3$ con 1 al centro e 0 altrove. La somma finale produce un unico kernel $3 \times 3$ equivalente}{figure.5.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Architettura Dettagliata}{23}{section.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Inverted Residual Block con ECA}{23}{subsection.5.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Schema del blocco Inverted Residual modificato con integrazione del modulo ECA dopo la Depthwise Convolution.}}{24}{figure.5.3}\protected@file@percent }
\newlabel{fig:inv_res_eca_detail}{{5.3}{24}{Schema del blocco Inverted Residual modificato con integrazione del modulo ECA dopo la Depthwise Convolution}{figure.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Scelte Architetturali Specifiche}{24}{subsection.5.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2.1}Stride Iniziale}{24}{subsubsection.5.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2.2}Attivazione: GELU vs ReLU}{25}{subsubsection.5.3.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Confronto tra ReLU (blu) e GELU (gialla). La natura liscia e non monotona della GELU vicino allo zero favorisce un flusso dei gradienti più robusto.}}{25}{figure.5.4}\protected@file@percent }
\newlabel{fig:gelu_vs_relu}{{5.4}{25}{Confronto tra ReLU (blu) e GELU (gialla). La natura liscia e non monotona della GELU vicino allo zero favorisce un flusso dei gradienti più robusto}{figure.5.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Implementazione del Deploy}{25}{section.5.4}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.1}{\ignorespaces Pseudo-codice per la fusione dei kernel (Switch to Deploy)}}{26}{lstlisting.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Confronto Standard Convolution vs Depthwise Separable (usata in MobileNet). La re-parametrizzazione permette di usare strutture complesse in training che collassano in efficienti depthwise/standard conv all'inferenza. L'immagine evidenzia la differenza fondamentale tra l'operazione spaziale densa e quella fattorizzata.}}{27}{figure.5.5}\protected@file@percent }
\newlabel{fig:conv_comparison}{{5.5}{27}{Confronto Standard Convolution vs Depthwise Separable (usata in MobileNet). La re-parametrizzazione permette di usare strutture complesse in training che collassano in efficienti depthwise/standard conv all'inferenza. L'immagine evidenzia la differenza fondamentale tra l'operazione spaziale densa e quella fattorizzata}{figure.5.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Configurazione Layer Dettagliata}{27}{subsection.5.4.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Configurazione Architetturale di MobileNetECA-Rep ($0.5\times $)}}{27}{table.5.1}\protected@file@percent }
\newlabel{tab:layer_config}{{5.1}{27}{Configurazione Architetturale di MobileNetECA-Rep ($0.5\times $)}{table.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}ECA Block: Implementazione Dettagliata}{28}{subsection.5.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Schema dettagliato del modulo ECA (Efficient Channel Attention). Viene eseguita una Global Average Pooling seguita da una convoluzione 1D con kernel adattivo e funzione sigmoidea.}}{28}{figure.5.6}\protected@file@percent }
\newlabel{fig:eca_detail}{{5.6}{28}{Schema dettagliato del modulo ECA (Efficient Channel Attention). Viene eseguita una Global Average Pooling seguita da una convoluzione 1D con kernel adattivo e funzione sigmoidea}{figure.5.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.2.1}Formula del Kernel Adattivo}{29}{subsubsection.5.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.2.2}Pipeline Operativa}{29}{subsubsection.5.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Configurazione Sperimentale}{29}{section.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}Strategie di Data Augmentation Avanzate}{29}{subsection.5.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.2}Protocollo di Training}{30}{subsection.5.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.3}Learning Rate Schedule}{30}{subsection.5.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Andamento del Learning Rate durante le 200 epoche di training (Cosine Annealing). Il tasso di apprendimento decresce dolcemente seguendo una curva cosinusoidale.}}{31}{figure.5.7}\protected@file@percent }
\newlabel{fig:lr_schedule}{{5.7}{31}{Andamento del Learning Rate durante le 200 epoche di training (Cosine Annealing). Il tasso di apprendimento decresce dolcemente seguendo una curva cosinusoidale}{figure.5.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Analisi della Complessità Teorica}{31}{section.5.6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Confronto Complessità MobileNetECA-Rep vs Baseline}}{31}{table.5.2}\protected@file@percent }
\newlabel{tab:complexity_new}{{5.2}{31}{Confronto Complessità MobileNetECA-Rep vs Baseline}{table.5.2}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {6}Analisi dei Risultati}{33}{chapter.6}\protected@file@percent }
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{cap:capitolo6}{{6}{33}{Analisi dei Risultati}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Ablation Study: Impatto delle Componenti}{33}{section.6.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Risultati dell'Ablation Study. Ogni riga rappresenta l'aggiunta di una componente rispetto alla precedente.}}{34}{table.6.1}\protected@file@percent }
\newlabel{tab:ablation}{{6.1}{34}{Risultati dell'Ablation Study. Ogni riga rappresenta l'aggiunta di una componente rispetto alla precedente}{table.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Dinamiche di Addestramento}{34}{subsection.6.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Analisi del Modello Finale (MobileNetECA-Rep)}{35}{section.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Matrice di Confusione}{35}{subsection.6.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Curve ROC e AUC}{35}{subsection.6.2.2}\protected@file@percent }
\citation{he2016deep}
\citation{he2016deep}
\citation{he2016deep}
\citation{he2016deep}
\citation{simonyan2014very}
\citation{sandler2018mobilenetv2}
\citation{sandler2018mobilenetv2}
\citation{ma2018shufflenet}
\citation{ding2021repvgg}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Analisi Qualitativa Avanzata}{36}{section.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Occlusion Sensitivity Analysis}{36}{subsection.6.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Analisi degli "Imperdonabili": High Confidence Errors}{36}{subsection.6.3.2}\protected@file@percent }
\citation{thriftynet2021}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Confronto Esteso con lo Stato dell'Arte}{37}{section.6.4}\protected@file@percent }
\newlabel{tab:comparison_sota}{{\caption@xref {tab:comparison_sota}{ on input line 4}}{37}{Confronto Esteso con lo Stato dell'Arte}{section.6.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces Confronto con lo Stato dell'Arte su CIFAR-10. Il nostro modello (MobileNetECA-Rep) ottiene risultati competitivi con una frazione dei parametri e dei FLOPs.}}{37}{table.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Confronto con lo Stato dell'Arte: Efficienza}{38}{section.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.1}Discussione}{38}{subsection.6.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.2}Analisi Empirica della Latenza e Implementabilità}{38}{subsection.6.5.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces Latenza di Inferenza media su CPU (1000 iterazioni)}}{39}{table.6.3}\protected@file@percent }
\newlabel{tab:latency}{{6.3}{39}{Latenza di Inferenza media su CPU (1000 iterazioni)}{table.6.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Analisi dell'Impatto degli Iperparametri}{39}{section.6.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.1}Interazione Learning Rate vs Weight Decay}{39}{subsection.6.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.2}Impatto della Capacità del Modello sulla Regolarizzazione}{40}{subsection.6.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Dinamiche di apprendimento per le diverse configurazioni. Si noti come la configurazione finale (rossa) converga a un'accuratezza superiore nonostante una loss iniziale più alta dovuta alla difficoltà introdotta dall'augmentation.}}{41}{figure.6.1}\protected@file@percent }
\newlabel{fig:accuracy_loss}{{6.1}{41}{Dinamiche di apprendimento per le diverse configurazioni. Si noti come la configurazione finale (rossa) converga a un'accuratezza superiore nonostante una loss iniziale più alta dovuta alla difficoltà introdotta dall'augmentation}{figure.6.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Dinamiche reali dell'addestramento finale (200 epoche). I dati evidenziano la stabilità della convergenza grazie allo schedule Cosine Annealing.}}{42}{figure.6.2}\protected@file@percent }
\newlabel{fig:real_dynamics}{{6.2}{42}{Dinamiche reali dell'addestramento finale (200 epoche). I dati evidenziano la stabilità della convergenza grazie allo schedule Cosine Annealing}{figure.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Matrice di Confusione del modello MobileNetECA-Rep sul Test Set.}}{42}{figure.6.3}\protected@file@percent }
\newlabel{fig:confusion_matrix}{{6.3}{42}{Matrice di Confusione del modello MobileNetECA-Rep sul Test Set}{figure.6.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Dettaglio delle curve ROC per le 10 classi (Zoom TPR 0.8-1.0). L'area sotto la curva (AUC) è prossima a 1.0 per tutte le classi, indicando un'eccellente separabilità.}}{43}{figure.6.4}\protected@file@percent }
\newlabel{fig:roc_zoomed}{{6.4}{43}{Dettaglio delle curve ROC per le 10 classi (Zoom TPR 0.8-1.0). L'area sotto la curva (AUC) è prossima a 1.0 per tutte le classi, indicando un'eccellente separabilità}{figure.6.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Mappe di Sensibilità all'Occlusione. (Riga 1) Immagine originale. (Riga 2) Heatmap: le aree rosse indicano le zone dove l'occlusione causa il maggior calo di confidenza. (Riga 3) Sovrapposizione. Si nota come il modello si concentri correttamente sulle caratteristiche discriminanti dell'oggetto (es. il muso del cane, la carrozzeria dell'auto) ignorando lo sfondo.}}{44}{figure.6.5}\protected@file@percent }
\newlabel{fig:occlusion_sensitivity}{{6.5}{44}{Mappe di Sensibilità all'Occlusione. (Riga 1) Immagine originale. (Riga 2) Heatmap: le aree rosse indicano le zone dove l'occlusione causa il maggior calo di confidenza. (Riga 3) Sovrapposizione. Si nota come il modello si concentri correttamente sulle caratteristiche discriminanti dell'oggetto (es. il muso del cane, la carrozzeria dell'auto) ignorando lo sfondo}{figure.6.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Errori ad Alta Confidenza ($>90\%$). Ogni immagine riporta la classe vera (True), quella predetta (Pred) e la confidenza. Molti di questi errori sono dovuti ad ambiguità visive estreme (es. prospettive insolite o oggetti parzialmente occlusi) che metterebbero in difficoltà anche un osservatore umano.}}{45}{figure.6.6}\protected@file@percent }
\newlabel{fig:top_errors}{{6.6}{45}{Errori ad Alta Confidenza ($>90\%$). Ogni immagine riporta la classe vera (True), quella predetta (Pred) e la confidenza. Molti di questi errori sono dovuti ad ambiguità visive estreme (es. prospettive insolite o oggetti parzialmente occlusi) che metterebbero in difficoltà anche un osservatore umano}{figure.6.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Efficiency Frontier: Accuratezza vs Numero di Parametri (scala logaritmica). Il nostro modello (stella rossa) si trova nettamente a sinistra (meno parametri) rispetto a modelli con accuratezza simile come ResNet-32 o VGG-16.}}{46}{figure.6.7}\protected@file@percent }
\newlabel{fig:efficiency}{{6.7}{46}{Efficiency Frontier: Accuratezza vs Numero di Parametri (scala logaritmica). Il nostro modello (stella rossa) si trova nettamente a sinistra (meno parametri) rispetto a modelli con accuratezza simile come ResNet-32 o VGG-16}{figure.6.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces Heatmap dell'accuratezza (Validation Accuracy) al variare di Learning Rate e Weight Decay. Si osserva che alti valori di LR richiedono un decay maggiore per stabilizzare l'addestramento.}}{47}{figure.6.8}\protected@file@percent }
\newlabel{fig:heatmap_lr_wd}{{6.8}{47}{Heatmap dell'accuratezza (Validation Accuracy) al variare di Learning Rate e Weight Decay. Si osserva che alti valori di LR richiedono un decay maggiore per stabilizzare l'addestramento}{figure.6.8}{}}
\@input{appendice.aux}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{Conclusioni e Sviluppi Futuri}{49}{appendix*.3}\protected@file@percent }
\@writefile{loa}{\addvspace {10\p@ }}
\citation{*}
\bibdata{bibliografia}
\bibcite{krizhevsky2009learning}{1}
\bibcite{hendrycks2016gaussian}{2}
\bibcite{he2016deep}{3}
\bibcite{howard2017mobilenets}{4}
\bibcite{sandler2018mobilenetv2}{5}
\bibcite{hu2018squeeze}{6}
\bibcite{wang2020eca}{7}
\@writefile{toc}{\contentsline {chapter}{Bibliografia}{56}{appendix*.14}\protected@file@percent }
\bibcite{ding2021repvgg}{8}
\bibcite{simonyan2014very}{9}
\bibcite{ma2018shufflenet}{10}
\bibcite{thriftynet2021}{11}
\bibcite{goodfellow2016deep}{12}
\bibcite{lecun2015deep}{13}
\bibcite{cubuk2019autoaugment}{14}
\bibcite{zhong2020random}{15}
\memsetcounter{lastsheet}{66}
\memsetcounter{lastpage}{57}
\gdef \@abspage@last{66}
