\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*{\memsetcounter}[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\bibstyle{ieeetr}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\babel@aux{italian}{}
\babel@aux{italian}{}
\@writefile{toc}{\contentsline {chapter}{Indice}{iii}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Elenco delle figure}{vii}{section*.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {1}Introduzione}{1}{chapter.1}\protected@file@percent }
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{cap:capitolo1}{{\M@TitleReference {1}{Introduzione}}{1}{Introduzione}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Contesto e Motivazione}{1}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Obiettivi della Tesi}{2}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Contributi del Lavoro}{2}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Struttura della Tesi}{4}{section.1.4}\protected@file@percent }
\citation{krizhevsky2009learning}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {2}Il Dataset CIFAR-10}{5}{chapter.2}\protected@file@percent }
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{cap:capitolo2}{{\M@TitleReference {2}{Il Dataset CIFAR-10}}{5}{Il Dataset CIFAR-10}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Descrizione del Dataset CIFAR-10}{5}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Struttura e Organizzazione dei Dati}{6}{subsection.2.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Esempi di Immagini dal Dataset CIFAR-10. La bassa risoluzione ($32 \times 32$) rende il compito di classificazione sfidante anche per l'occhio umano, specialmente per classi visivamente simili.\relax }}{7}{figure.2.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:cifar10_sample}{{\M@TitleReference {2.1}{Esempi di Immagini dal Dataset CIFAR-10. La bassa risoluzione ($32 \times 32$) rende il compito di classificazione sfidante anche per l'occhio umano, specialmente per classi visivamente simili.\relax }}{7}{Esempi di Immagini dal Dataset CIFAR-10. La bassa risoluzione ($32 \times 32$) rende il compito di classificazione sfidante anche per l'occhio umano, specialmente per classi visivamente simili.\relax }{figure.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Preprocessing dei Dati}{7}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Normalizzazione}{7}{subsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Data Augmentation Standard}{8}{subsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Sfide e Limitazioni}{8}{section.2.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {3}Fondamenti delle Reti Neurali}{10}{chapter.3}\protected@file@percent }
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{cap:capitolo3}{{\M@TitleReference {3}{Fondamenti delle Reti Neurali}}{10}{Fondamenti delle Reti Neurali}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Apprendimento Automatico}{10}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Paradigmi di Apprendimento}{10}{subsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Il Perceptron e il Neurone Artificiale}{11}{subsection.3.1.2}\protected@file@percent }
\citation{hendrycks2016gaussian}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Funzioni di Attivazione}{12}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Sigmoide e Tanh}{12}{subsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}ReLU (Rectified Linear Unit)}{12}{subsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}GELU (Gaussian Error Linear Unit)}{13}{subsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Addestramento della Rete}{13}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Funzione di Costo (Loss Function)}{13}{subsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Algoritmo di Backpropagation}{13}{subsection.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Ottimizzazione e Regolarizzazione}{14}{section.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Ottimizzatori}{14}{subsection.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1.1}Stochastic Gradient Descent (SGD)}{14}{subsubsection.3.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1.2}SGD con Momentum}{14}{subsubsection.3.4.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1.3}Adam (Adaptive Moment Estimation)}{15}{subsubsection.3.4.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Tecniche di Regolarizzazione}{15}{subsection.3.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2.1}Batch Normalization}{15}{subsubsection.3.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2.2}Dropout}{15}{subsubsection.3.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2.3}Weight Decay (L2 Regularization)}{15}{subsubsection.3.4.2.3}\protected@file@percent }
\citation{he2016deep}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {4}Stato dell'Arte e Tecniche di Efficienza}{16}{chapter.4}\protected@file@percent }
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{cap:capitolo4}{{\M@TitleReference {4}{Stato dell'Arte e Tecniche di Efficienza}}{16}{Stato dell'Arte e Tecniche di Efficienza}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Evoluzione delle Architetture CNN}{16}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Dalle Origini al Deep Learning Moderno}{16}{subsection.4.1.1}\protected@file@percent }
\citation{howard2017mobilenets}
\citation{sandler2018mobilenetv2}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Architetture Efficienti per Mobile}{17}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Depthwise Separable Convolutions}{17}{subsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}MobileNetV2: Inverted Residuals}{17}{subsection.4.2.2}\protected@file@percent }
\citation{hu2018squeeze}
\citation{wang2020eca}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Meccanismi di Attenzione}{18}{section.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Squeeze-and-Excitation (SE)}{18}{subsection.4.3.1}\protected@file@percent }
\citation{ding2021repvgg}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Efficient Channel Attention (ECA)}{19}{subsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Structural Reparameterization (RepVGG)}{19}{section.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Il Principio di Equivalenza}{19}{subsection.4.4.1}\protected@file@percent }
\citation{ding2021repvgg}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {5}Progettazione e Architettura del Sistema}{20}{chapter.5}\protected@file@percent }
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{cap:capitolo5}{{\M@TitleReference {5}{Progettazione e Architettura del Sistema}}{20}{Progettazione e Architettura del Sistema}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Panoramica del Sistema}{20}{section.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Architettura MobileNetECARep. I blocchi azzurri rappresentano i tensori intermedi. Il dettaglio mostra il modulo RepInvertedResidual con integrazione di Efficient Channel Attention (ECA) e la struttura di convoluzione riparametrizzabile (RepConv).\relax }}{21}{figure.5.1}\protected@file@percent }
\newlabel{fig:mobilenet_eca_rep_overview}{{\M@TitleReference {5.1}{Architettura MobileNetECARep. I blocchi azzurri rappresentano i tensori intermedi. Il dettaglio mostra il modulo RepInvertedResidual con integrazione di Efficient Channel Attention (ECA) e la struttura di convoluzione riparametrizzabile (RepConv).\relax }}{21}{Architettura MobileNetECARep. I blocchi azzurri rappresentano i tensori intermedi. Il dettaglio mostra il modulo RepInvertedResidual con integrazione di Efficient Channel Attention (ECA) e la struttura di convoluzione riparametrizzabile (RepConv).\relax }{figure.5.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Structural Reparameterization}{21}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Concetto Chiave: Training vs Inference}{21}{subsection.5.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Fusione Matematica dei Kernel: Derivazione Completa}{22}{subsection.5.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Visualizzazione della fusione dei kernel. Il kernel $1 \times 1$ viene portato a $3 \times 3$ tramite zero-padding, mentre l'identità diventa un kernel $3 \times 3$ con 1 al centro e 0 altrove. La somma finale produce un unico kernel $3 \times 3$ equivalente.\relax }}{23}{figure.5.2}\protected@file@percent }
\newlabel{fig:kernel_fusion_viz}{{\M@TitleReference {5.2}{Visualizzazione della fusione dei kernel. Il kernel $1 \times 1$ viene portato a $3 \times 3$ tramite zero-padding, mentre l'identità diventa un kernel $3 \times 3$ con 1 al centro e 0 altrove. La somma finale produce un unico kernel $3 \times 3$ equivalente.\relax }}{23}{Visualizzazione della fusione dei kernel. Il kernel $1 \times 1$ viene portato a $3 \times 3$ tramite zero-padding, mentre l'identità diventa un kernel $3 \times 3$ con 1 al centro e 0 altrove. La somma finale produce un unico kernel $3 \times 3$ equivalente.\relax }{figure.5.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Architettura Dettagliata}{23}{section.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Inverted Residual Block con ECA}{23}{subsection.5.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Schema del blocco Inverted Residual modificato con integrazione del modulo ECA dopo la Depthwise Convolution.\relax }}{24}{figure.5.3}\protected@file@percent }
\newlabel{fig:inv_res_eca_detail}{{\M@TitleReference {5.3}{Schema del blocco Inverted Residual modificato con integrazione del modulo ECA dopo la Depthwise Convolution.\relax }}{24}{Schema del blocco Inverted Residual modificato con integrazione del modulo ECA dopo la Depthwise Convolution.\relax }{figure.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Scelte Architetturali Specifiche}{24}{subsection.5.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2.1}Stride Iniziale}{24}{subsubsection.5.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2.2}Attivazione: GELU vs ReLU}{25}{subsubsection.5.3.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Confronto tra ReLU (blu) e GELU (gialla). La natura liscia e non monotona della GELU vicino allo zero favorisce un flusso dei gradienti più robusto.\relax }}{25}{figure.5.4}\protected@file@percent }
\newlabel{fig:gelu_vs_relu}{{\M@TitleReference {5.4}{Confronto tra ReLU (blu) e GELU (gialla). La natura liscia e non monotona della GELU vicino allo zero favorisce un flusso dei gradienti più robusto.\relax }}{25}{Confronto tra ReLU (blu) e GELU (gialla). La natura liscia e non monotona della GELU vicino allo zero favorisce un flusso dei gradienti più robusto.\relax }{figure.5.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Implementazione del Deploy}{25}{section.5.4}\protected@file@percent }
\citation{ding2021repvgg}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.1}Pseudo-codice per la fusione dei kernel (Switch to Deploy)}{26}{lstlisting.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Confronto Standard Convolution vs Depthwise Separable (usata in MobileNet). La re-parametrizzazione permette di usare strutture complesse in training che collassano in efficienti depthwise/standard conv all'inferenza. L'immagine evidenzia la differenza fondamentale tra l'operazione spaziale densa e quella fattorizzata.\relax }}{27}{figure.5.5}\protected@file@percent }
\newlabel{fig:conv_comparison}{{\M@TitleReference {5.5}{Confronto Standard Convolution vs Depthwise Separable (usata in MobileNet). La re-parametrizzazione permette di usare strutture complesse in training che collassano in efficienti depthwise/standard conv all'inferenza. L'immagine evidenzia la differenza fondamentale tra l'operazione spaziale densa e quella fattorizzata.\relax }}{27}{Confronto Standard Convolution vs Depthwise Separable (usata in MobileNet). La re-parametrizzazione permette di usare strutture complesse in training che collassano in efficienti depthwise/standard conv all'inferenza. L'immagine evidenzia la differenza fondamentale tra l'operazione spaziale densa e quella fattorizzata.\relax }{figure.5.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Configurazione Layer Dettagliata}{27}{subsection.5.4.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Configurazione Architetturale di MobileNetECA-Rep ($0.5\times $)\relax }}{27}{table.5.1}\protected@file@percent }
\newlabel{tab:layer_config}{{\M@TitleReference {5.1}{Configurazione Architetturale di MobileNetECA-Rep ($0.5\times $)\relax }}{27}{Configurazione Architetturale di MobileNetECA-Rep ($0.5\times $)\relax }{table.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}ECA Block: Implementazione Dettagliata}{28}{subsection.5.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Schema dettagliato del modulo ECA (Efficient Channel Attention). Viene eseguita una Global Average Pooling seguita da una convoluzione 1D con kernel adattivo e funzione sigmoidea.\relax }}{28}{figure.5.6}\protected@file@percent }
\newlabel{fig:eca_detail}{{\M@TitleReference {5.6}{Schema dettagliato del modulo ECA (Efficient Channel Attention). Viene eseguita una Global Average Pooling seguita da una convoluzione 1D con kernel adattivo e funzione sigmoidea.\relax }}{28}{Schema dettagliato del modulo ECA (Efficient Channel Attention). Viene eseguita una Global Average Pooling seguita da una convoluzione 1D con kernel adattivo e funzione sigmoidea.\relax }{figure.5.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.2.1}Perché ECA e non SE?}{29}{subsubsection.5.4.2.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Confronto parametri aggiuntivi: SE Block vs ECA Block per $C=208$ canali.\relax }}{29}{table.5.2}\protected@file@percent }
\newlabel{tab:se_vs_eca}{{\M@TitleReference {5.2}{Confronto parametri aggiuntivi: SE Block vs ECA Block per $C=208$ canali.\relax }}{29}{Confronto parametri aggiuntivi: SE Block vs ECA Block per $C=208$ canali.\relax }{table.5.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.2.2}Formula del Kernel Adattivo}{29}{subsubsection.5.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.2.3}Implementazione PyTorch}{30}{subsubsection.5.4.2.3}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.2}Implementazione del blocco ECA (Efficient Channel Attention)}{30}{lstlisting.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.2.4}Pipeline Operativa}{30}{subsubsection.5.4.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Configurazione Sperimentale}{31}{section.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}Strategie di Data Augmentation Avanzate}{31}{subsection.5.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.2}Protocollo di Training}{32}{subsection.5.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Protocollo di Validazione.}{32}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.3}Learning Rate Schedule}{32}{subsection.5.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Andamento del Learning Rate durante le 200 epoche di training (Cosine Annealing). Il tasso di apprendimento decresce dolcemente seguendo una curva cosinusoidale.\relax }}{33}{figure.5.7}\protected@file@percent }
\newlabel{fig:lr_schedule}{{\M@TitleReference {5.7}{Andamento del Learning Rate durante le 200 epoche di training (Cosine Annealing). Il tasso di apprendimento decresce dolcemente seguendo una curva cosinusoidale.\relax }}{33}{Andamento del Learning Rate durante le 200 epoche di training (Cosine Annealing). Il tasso di apprendimento decresce dolcemente seguendo una curva cosinusoidale.\relax }{figure.5.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Analisi della Complessità Teorica}{33}{section.5.6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces Confronto Complessità MobileNetECA-Rep vs Baseline\relax }}{33}{table.5.3}\protected@file@percent }
\newlabel{tab:complexity_new}{{\M@TitleReference {5.3}{Confronto Complessità MobileNetECA-Rep vs Baseline\relax }}{33}{Confronto Complessità MobileNetECA-Rep vs Baseline\relax }{table.5.3}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {6}Analisi dei Risultati}{34}{chapter.6}\protected@file@percent }
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{cap:capitolo6}{{\M@TitleReference {6}{Analisi dei Risultati}}{34}{Analisi dei Risultati}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Ablation Study: Impatto delle Componenti}{34}{section.6.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Risultati dell'Ablation Study. Ogni riga rappresenta l'aggiunta di una componente rispetto alla precedente. L'accuratezza di test è misurata una sola volta sul modello selezionato tramite early stopping sulla validazione.\relax }}{35}{table.6.1}\protected@file@percent }
\newlabel{tab:ablation}{{\M@TitleReference {6.1}{Risultati dell'Ablation Study. Ogni riga rappresenta l'aggiunta di una componente rispetto alla precedente. L'accuratezza di test è misurata una sola volta sul modello selezionato tramite early stopping sulla validazione.\relax }}{35}{Risultati dell'Ablation Study. Ogni riga rappresenta l'aggiunta di una componente rispetto alla precedente. L'accuratezza di test è misurata una sola volta sul modello selezionato tramite early stopping sulla validazione.\relax }{table.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Dinamiche di Addestramento}{35}{subsection.6.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Analisi del Modello Finale (MobileNetECA-Rep)}{36}{section.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Matrice di Confusione}{36}{subsection.6.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Curve ROC e AUC}{37}{subsection.6.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Dettaglio delle curve ROC per le 10 classi (Zoom TPR 0.8-1.0). L'area sotto la curva (AUC) è prossima a 1.0 per tutte le classi, indicando un'eccellente separabilità.\relax }}{37}{figure.6.4}\protected@file@percent }
\newlabel{fig:roc_zoomed}{{\M@TitleReference {6.4}{Dettaglio delle curve ROC per le 10 classi (Zoom TPR 0.8-1.0). L'area sotto la curva (AUC) è prossima a 1.0 per tutte le classi, indicando un'eccellente separabilità.\relax }}{37}{Dettaglio delle curve ROC per le 10 classi (Zoom TPR 0.8-1.0). L'area sotto la curva (AUC) è prossima a 1.0 per tutte le classi, indicando un'eccellente separabilità.\relax }{figure.6.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Analisi Qualitativa Avanzata}{38}{section.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Occlusion Sensitivity Analysis}{38}{subsection.6.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Mappe di Sensibilità all'Occlusione. (Riga 1) Immagine originale. (Riga 2) Heatmap: le aree rosse indicano le zone dove l'occlusione causa il maggior calo di confidenza. (Riga 3) Sovrapposizione. Si nota come il modello si concentri correttamente sulle caratteristiche discriminanti dell'oggetto (es. il muso del cane, la carrozzeria dell'auto) ignorando lo sfondo.\relax }}{39}{figure.6.5}\protected@file@percent }
\newlabel{fig:occlusion_sensitivity}{{\M@TitleReference {6.5}{Mappe di Sensibilità all'Occlusione. (Riga 1) Immagine originale. (Riga 2) Heatmap: le aree rosse indicano le zone dove l'occlusione causa il maggior calo di confidenza. (Riga 3) Sovrapposizione. Si nota come il modello si concentri correttamente sulle caratteristiche discriminanti dell'oggetto (es. il muso del cane, la carrozzeria dell'auto) ignorando lo sfondo.\relax }}{39}{Mappe di Sensibilità all'Occlusione. (Riga 1) Immagine originale. (Riga 2) Heatmap: le aree rosse indicano le zone dove l'occlusione causa il maggior calo di confidenza. (Riga 3) Sovrapposizione. Si nota come il modello si concentri correttamente sulle caratteristiche discriminanti dell'oggetto (es. il muso del cane, la carrozzeria dell'auto) ignorando lo sfondo.\relax }{figure.6.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Analisi degli "Imperdonabili": High Confidence Errors}{39}{subsection.6.3.2}\protected@file@percent }
\citation{he2016deep}
\citation{he2016deep}
\citation{he2016deep}
\citation{he2016deep}
\citation{simonyan2014very}
\citation{sandler2018mobilenetv2}
\citation{sandler2018mobilenetv2}
\citation{ma2018shufflenet}
\citation{ding2021repvgg}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Errori ad Alta Confidenza ($>90\%$). Ogni immagine riporta la classe vera (True), quella predetta (Pred) e la confidenza. Molti di questi errori sono dovuti ad ambiguità visive estreme (es. prospettive insolite o oggetti parzialmente occlusi) che metterebbero in difficoltà anche un osservatore umano.\relax }}{40}{figure.6.6}\protected@file@percent }
\newlabel{fig:top_errors}{{\M@TitleReference {6.6}{Errori ad Alta Confidenza ($>90\%$). Ogni immagine riporta la classe vera (True), quella predetta (Pred) e la confidenza. Molti di questi errori sono dovuti ad ambiguità visive estreme (es. prospettive insolite o oggetti parzialmente occlusi) che metterebbero in difficoltà anche un osservatore umano.\relax }}{40}{Errori ad Alta Confidenza ($>90\%$). Ogni immagine riporta la classe vera (True), quella predetta (Pred) e la confidenza. Molti di questi errori sono dovuti ad ambiguità visive estreme (es. prospettive insolite o oggetti parzialmente occlusi) che metterebbero in difficoltà anche un osservatore umano.\relax }{figure.6.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Confronto Esteso con lo Stato dell'Arte}{40}{section.6.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces Confronto con lo Stato dell'Arte su CIFAR-10. Il nostro modello (MobileNetECA-Rep) ottiene risultati competitivi con una frazione dei parametri e dei FLOPs.\relax }}{41}{table.6.2}\protected@file@percent }
\newlabel{tab:comparison_sota}{{\M@TitleReference {6.2}{Confronto con lo Stato dell'Arte su CIFAR-10. Il nostro modello (MobileNetECA-Rep) ottiene risultati competitivi con una frazione dei parametri e dei FLOPs.\relax }}{41}{Confronto con lo Stato dell'Arte su CIFAR-10. Il nostro modello (MobileNetECA-Rep) ottiene risultati competitivi con una frazione dei parametri e dei FLOPs.\relax }{table.6.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Confronto con lo Stato dell'Arte: Efficienza}{42}{section.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.1}Discussione}{42}{subsection.6.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.2}Analisi Empirica della Latenza e Implementabilità}{42}{subsection.6.5.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces Latenza di Inferenza media su CPU (1000 iterazioni)\relax }}{42}{table.6.3}\protected@file@percent }
\newlabel{tab:latency}{{\M@TitleReference {6.3}{Latenza di Inferenza media su CPU (1000 iterazioni)\relax }}{42}{Latenza di Inferenza media su CPU (1000 iterazioni)\relax }{table.6.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Nota sulla Variabilità.}{42}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Analisi dell'Impatto degli Iperparametri}{43}{section.6.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Nota Metodologica.}{43}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.1}Interazione Learning Rate vs Weight Decay}{43}{subsection.6.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.2}Impatto della Capacità del Modello sulla Regolarizzazione}{44}{subsection.6.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Dinamiche di apprendimento per le diverse configurazioni. Si noti come la configurazione finale (rossa) converga a un'accuratezza superiore nonostante una loss iniziale più alta dovuta alla difficoltà introdotta dall'augmentation.\relax }}{45}{figure.6.1}\protected@file@percent }
\newlabel{fig:accuracy_loss}{{\M@TitleReference {6.1}{Dinamiche di apprendimento per le diverse configurazioni. Si noti come la configurazione finale (rossa) converga a un'accuratezza superiore nonostante una loss iniziale più alta dovuta alla difficoltà introdotta dall'augmentation.\relax }}{45}{Dinamiche di apprendimento per le diverse configurazioni. Si noti come la configurazione finale (rossa) converga a un'accuratezza superiore nonostante una loss iniziale più alta dovuta alla difficoltà introdotta dall'augmentation.\relax }{figure.6.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Dinamiche reali dell'addestramento finale (200 epoche). I dati evidenziano la stabilità della convergenza grazie allo schedule Cosine Annealing.\relax }}{46}{figure.6.2}\protected@file@percent }
\newlabel{fig:real_dynamics}{{\M@TitleReference {6.2}{Dinamiche reali dell'addestramento finale (200 epoche). I dati evidenziano la stabilità della convergenza grazie allo schedule Cosine Annealing.\relax }}{46}{Dinamiche reali dell'addestramento finale (200 epoche). I dati evidenziano la stabilità della convergenza grazie allo schedule Cosine Annealing.\relax }{figure.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Matrice di Confusione del modello MobileNetECA-Rep sul Test Set.\relax }}{46}{figure.6.3}\protected@file@percent }
\newlabel{fig:confusion_matrix}{{\M@TitleReference {6.3}{Matrice di Confusione del modello MobileNetECA-Rep sul Test Set.\relax }}{46}{Matrice di Confusione del modello MobileNetECA-Rep sul Test Set.\relax }{figure.6.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Efficiency Frontier: Accuratezza vs Numero di Parametri (scala logaritmica). Il nostro modello (stella rossa) si trova nettamente a sinistra (meno parametri) rispetto a modelli con accuratezza simile come ResNet-32 o VGG-16.\relax }}{47}{figure.6.7}\protected@file@percent }
\newlabel{fig:efficiency}{{\M@TitleReference {6.7}{Efficiency Frontier: Accuratezza vs Numero di Parametri (scala logaritmica). Il nostro modello (stella rossa) si trova nettamente a sinistra (meno parametri) rispetto a modelli con accuratezza simile come ResNet-32 o VGG-16.\relax }}{47}{Efficiency Frontier: Accuratezza vs Numero di Parametri (scala logaritmica). Il nostro modello (stella rossa) si trova nettamente a sinistra (meno parametri) rispetto a modelli con accuratezza simile come ResNet-32 o VGG-16.\relax }{figure.6.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces Heatmap dell'accuratezza (Validation Accuracy) al variare di Learning Rate e Weight Decay. Si osserva che alti valori di LR richiedono un decay maggiore per stabilizzare l'addestramento.\relax }}{48}{figure.6.8}\protected@file@percent }
\newlabel{fig:heatmap_lr_wd}{{\M@TitleReference {6.8}{Heatmap dell'accuratezza (Validation Accuracy) al variare di Learning Rate e Weight Decay. Si osserva che alti valori di LR richiedono un decay maggiore per stabilizzare l'addestramento.\relax }}{48}{Heatmap dell'accuratezza (Validation Accuracy) al variare di Learning Rate e Weight Decay. Si osserva che alti valori di LR richiedono un decay maggiore per stabilizzare l'addestramento.\relax }{figure.6.8}{}}
\@input{appendice.aux}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{Conclusioni e Sviluppi Futuri}{50}{appendix*.6}\protected@file@percent }
\@writefile{loa}{\addvspace {10\p@ }}
\bibdata{bibliografia}
\bibcite{krizhevsky2009learning}{1}
\bibcite{hendrycks2016gaussian}{2}
\bibcite{he2016deep}{3}
\bibcite{howard2017mobilenets}{4}
\bibcite{sandler2018mobilenetv2}{5}
\bibcite{hu2018squeeze}{6}
\bibcite{wang2020eca}{7}
\@writefile{toc}{\contentsline {chapter}{Bibliografia}{57}{appendix*.17}\protected@file@percent }
\bibcite{ding2021repvgg}{8}
\bibcite{simonyan2014very}{9}
\bibcite{ma2018shufflenet}{10}
\memsetcounter{lastsheet}{67}
\memsetcounter{lastpage}{58}
\gdef \@abspage@last{67}
