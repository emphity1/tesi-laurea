\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*{\memsetcounter}[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\bibstyle{ieeetr}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\babel@aux{italian}{}
\babel@aux{italian}{}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{Introduzione}{iii}{chapter*.1}\protected@file@percent }
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{Indice}{iv}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Elenco delle figure}{viii}{section*.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {1}Introduzione}{1}{chapter.1}\protected@file@percent }
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{cap:capitolo1}{{\M@TitleReference {1}{Introduzione}}{1}{Introduzione}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Contesto e Motivazione}{1}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Obiettivi della Tesi}{2}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Contributi del Lavoro}{2}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Struttura della Tesi}{3}{section.1.4}\protected@file@percent }
\citation{krizhevsky2009learning}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {2}Il Dataset CIFAR-10}{5}{chapter.2}\protected@file@percent }
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{cap:capitolo2}{{\M@TitleReference {2}{Il Dataset CIFAR-10}}{5}{Il Dataset CIFAR-10}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Descrizione del Dataset CIFAR-10}{5}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Struttura e Organizzazione dei Dati}{6}{subsection.2.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Esempi di Immagini dal Dataset CIFAR-10. La bassa risoluzione ($32 \times 32$) rende il compito di classificazione sfidante anche per l'occhio umano, specialmente per classi visivamente simili.\relax }}{7}{figure.2.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:cifar10_sample}{{\M@TitleReference {2.1}{Esempi di Immagini dal Dataset CIFAR-10. La bassa risoluzione ($32 \times 32$) rende il compito di classificazione sfidante anche per l'occhio umano, specialmente per classi visivamente simili.\relax }}{7}{Esempi di Immagini dal Dataset CIFAR-10. La bassa risoluzione ($32 \times 32$) rende il compito di classificazione sfidante anche per l'occhio umano, specialmente per classi visivamente simili.\relax }{figure.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Preprocessing dei Dati}{7}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Normalizzazione}{7}{subsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Data Augmentation Standard}{8}{subsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Sfide e Limitazioni}{8}{section.2.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {3}Fondamenti delle Reti Neurali}{10}{chapter.3}\protected@file@percent }
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{cap:capitolo3}{{\M@TitleReference {3}{Fondamenti delle Reti Neurali}}{10}{Fondamenti delle Reti Neurali}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Apprendimento Automatico}{10}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Paradigmi di Apprendimento}{10}{subsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Il Perceptron e il Neurone Artificiale}{11}{subsection.3.1.2}\protected@file@percent }
\citation{hendrycks2016gaussian}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Funzioni di Attivazione}{12}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Sigmoide e Tanh}{12}{subsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}ReLU (Rectified Linear Unit)}{12}{subsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}GELU (Gaussian Error Linear Unit)}{13}{subsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Addestramento della Rete}{13}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Funzione di Costo (Loss Function)}{13}{subsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Algoritmo di Backpropagation}{13}{subsection.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Ottimizzazione e Regolarizzazione}{14}{section.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Ottimizzatori}{14}{subsection.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1.1}Stochastic Gradient Descent (SGD)}{14}{subsubsection.3.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1.2}SGD con Momentum}{14}{subsubsection.3.4.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1.3}Adam (Adaptive Moment Estimation)}{15}{subsubsection.3.4.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Tecniche di Regolarizzazione}{15}{subsection.3.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2.1}Batch Normalization}{15}{subsubsection.3.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2.2}Dropout}{15}{subsubsection.3.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2.3}Weight Decay (L2 Regularization)}{15}{subsubsection.3.4.2.3}\protected@file@percent }
\citation{he2016deep}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {4}Stato dell'Arte e Tecniche di Efficienza}{16}{chapter.4}\protected@file@percent }
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{cap:capitolo4}{{\M@TitleReference {4}{Stato dell'Arte e Tecniche di Efficienza}}{16}{Stato dell'Arte e Tecniche di Efficienza}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Evoluzione delle Architetture CNN}{16}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Dalle Origini al Deep Learning Moderno}{16}{subsection.4.1.1}\protected@file@percent }
\citation{howard2017mobilenets}
\citation{sandler2018mobilenetv2}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Architetture Efficienti per Mobile}{17}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Depthwise Separable Convolutions}{17}{subsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}MobileNetV2: Inverted Residuals}{17}{subsection.4.2.2}\protected@file@percent }
\citation{hu2018squeeze}
\citation{wang2020eca}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Meccanismi di Attenzione}{18}{section.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Squeeze-and-Excitation (SE)}{18}{subsection.4.3.1}\protected@file@percent }
\citation{ding2021repvgg}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Efficient Channel Attention (ECA)}{19}{subsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Structural Reparameterization (RepVGG)}{19}{section.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Il Principio di Equivalenza}{19}{subsection.4.4.1}\protected@file@percent }
\citation{ding2021repvgg}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {5}Progettazione e Architettura del Sistema}{20}{chapter.5}\protected@file@percent }
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{cap:capitolo5}{{\M@TitleReference {5}{Progettazione e Architettura del Sistema}}{20}{Progettazione e Architettura del Sistema}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Panoramica del Sistema}{20}{section.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Architettura MobileNetECARep. I blocchi azzurri rappresentano i tensori intermedi. Il dettaglio mostra il modulo RepInvertedResidual con integrazione di Efficient Channel Attention (ECA) e la struttura di convoluzione riparametrizzabile (RepConv).\relax }}{21}{figure.5.1}\protected@file@percent }
\newlabel{fig:mobilenet_eca_rep_overview}{{\M@TitleReference {5.1}{Architettura MobileNetECARep. I blocchi azzurri rappresentano i tensori intermedi. Il dettaglio mostra il modulo RepInvertedResidual con integrazione di Efficient Channel Attention (ECA) e la struttura di convoluzione riparametrizzabile (RepConv).\relax }}{21}{Architettura MobileNetECARep. I blocchi azzurri rappresentano i tensori intermedi. Il dettaglio mostra il modulo RepInvertedResidual con integrazione di Efficient Channel Attention (ECA) e la struttura di convoluzione riparametrizzabile (RepConv).\relax }{figure.5.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Structural Reparameterization}{21}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Concetto Chiave: Training vs Inference}{21}{subsection.5.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Fusione Matematica dei Kernel: Derivazione Completa}{22}{subsection.5.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Visualizzazione della fusione dei kernel. Il kernel $1 \times 1$ viene portato a $3 \times 3$ tramite zero-padding, mentre l'identità diventa un kernel $3 \times 3$ con 1 al centro e 0 altrove. La somma finale produce un unico kernel $3 \times 3$ equivalente.\relax }}{23}{figure.5.2}\protected@file@percent }
\newlabel{fig:kernel_fusion_viz}{{\M@TitleReference {5.2}{Visualizzazione della fusione dei kernel. Il kernel $1 \times 1$ viene portato a $3 \times 3$ tramite zero-padding, mentre l'identità diventa un kernel $3 \times 3$ con 1 al centro e 0 altrove. La somma finale produce un unico kernel $3 \times 3$ equivalente.\relax }}{23}{Visualizzazione della fusione dei kernel. Il kernel $1 \times 1$ viene portato a $3 \times 3$ tramite zero-padding, mentre l'identità diventa un kernel $3 \times 3$ con 1 al centro e 0 altrove. La somma finale produce un unico kernel $3 \times 3$ equivalente.\relax }{figure.5.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Architettura Dettagliata}{23}{section.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Inverted Residual Block con ECA}{23}{subsection.5.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Schema del blocco Inverted Residual modificato con integrazione del modulo ECA dopo la Depthwise Convolution.\relax }}{24}{figure.5.3}\protected@file@percent }
\newlabel{fig:inv_res_eca_detail}{{\M@TitleReference {5.3}{Schema del blocco Inverted Residual modificato con integrazione del modulo ECA dopo la Depthwise Convolution.\relax }}{24}{Schema del blocco Inverted Residual modificato con integrazione del modulo ECA dopo la Depthwise Convolution.\relax }{figure.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Scelte Architetturali Specifiche}{24}{subsection.5.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2.1}Stride Iniziale}{24}{subsubsection.5.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2.2}Attivazione: GELU vs ReLU}{24}{subsubsection.5.3.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Confronto tra ReLU (rossa) e GELU (blu). La natura liscia e non monotona della GELU vicino allo zero favorisce un flusso dei gradienti più robusto.\relax }}{25}{figure.5.4}\protected@file@percent }
\newlabel{fig:gelu_vs_relu}{{\M@TitleReference {5.4}{Confronto tra ReLU (rossa) e GELU (blu). La natura liscia e non monotona della GELU vicino allo zero favorisce un flusso dei gradienti più robusto.\relax }}{25}{Confronto tra ReLU (rossa) e GELU (blu). La natura liscia e non monotona della GELU vicino allo zero favorisce un flusso dei gradienti più robusto.\relax }{figure.5.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Implementazione del Deploy}{25}{section.5.4}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.1}Pseudo-codice per la fusione dei kernel (Switch to Deploy)}{25}{lstlisting.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Confronto Standard Convolution vs Depthwise Separable (usata in MobileNet). La re-parametrizzazione permette di usare strutture complesse in training che collassano in efficienti depthwise/standard conv all'inferenza. L'immagine evidenzia la differenza fondamentale tra l'operazione spaziale densa e quella fattorizzata.\relax }}{26}{figure.5.5}\protected@file@percent }
\newlabel{fig:conv_comparison}{{\M@TitleReference {5.5}{Confronto Standard Convolution vs Depthwise Separable (usata in MobileNet). La re-parametrizzazione permette di usare strutture complesse in training che collassano in efficienti depthwise/standard conv all'inferenza. L'immagine evidenzia la differenza fondamentale tra l'operazione spaziale densa e quella fattorizzata.\relax }}{26}{Confronto Standard Convolution vs Depthwise Separable (usata in MobileNet). La re-parametrizzazione permette di usare strutture complesse in training che collassano in efficienti depthwise/standard conv all'inferenza. L'immagine evidenzia la differenza fondamentale tra l'operazione spaziale densa e quella fattorizzata.\relax }{figure.5.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}ECA Block: Implementazione Dettagliata}{27}{subsection.5.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Schema dettagliato del modulo ECA (Efficient Channel Attention). Viene eseguita una Global Average Pooling seguita da una convoluzione 1D con kernel adattivo e funzione sigmoidea.\relax }}{27}{figure.5.6}\protected@file@percent }
\newlabel{fig:eca_detail}{{\M@TitleReference {5.6}{Schema dettagliato del modulo ECA (Efficient Channel Attention). Viene eseguita una Global Average Pooling seguita da una convoluzione 1D con kernel adattivo e funzione sigmoidea.\relax }}{27}{Schema dettagliato del modulo ECA (Efficient Channel Attention). Viene eseguita una Global Average Pooling seguita da una convoluzione 1D con kernel adattivo e funzione sigmoidea.\relax }{figure.5.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.1.1}Formula del Kernel Adattivo}{27}{subsubsection.5.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.1.2}Pipeline Operativa}{28}{subsubsection.5.4.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Configurazione Sperimentale}{28}{section.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}Strategie di Data Augmentation Avanzate}{28}{subsection.5.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.2}Protocollo di Training}{29}{subsection.5.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.3}Learning Rate Schedule}{29}{subsection.5.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Andamento del Learning Rate durante le 200 epoche di training (Cosine Annealing). Il tasso di apprendimento decresce dolcemente seguendo una curva cosinusoidale.\relax }}{29}{figure.5.7}\protected@file@percent }
\newlabel{fig:lr_schedule}{{\M@TitleReference {5.7}{Andamento del Learning Rate durante le 200 epoche di training (Cosine Annealing). Il tasso di apprendimento decresce dolcemente seguendo una curva cosinusoidale.\relax }}{29}{Andamento del Learning Rate durante le 200 epoche di training (Cosine Annealing). Il tasso di apprendimento decresce dolcemente seguendo una curva cosinusoidale.\relax }{figure.5.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Analisi della Complessità Teorica}{30}{section.5.6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Confronto Complessità MobileNetECA-Rep vs Baseline\relax }}{30}{table.5.1}\protected@file@percent }
\newlabel{tab:complexity_new}{{\M@TitleReference {5.1}{Confronto Complessità MobileNetECA-Rep vs Baseline\relax }}{30}{Confronto Complessità MobileNetECA-Rep vs Baseline\relax }{table.5.1}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {6}Analisi dei Risultati}{31}{chapter.6}\protected@file@percent }
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{cap:capitolo6}{{\M@TitleReference {6}{Analisi dei Risultati}}{31}{Analisi dei Risultati}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Ablation Study: Impatto delle Componenti}{31}{section.6.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Risultati dell'Ablation Study. Ogni riga rappresenta l'aggiunta di una componente rispetto alla precedente.\relax }}{32}{table.6.1}\protected@file@percent }
\newlabel{tab:ablation}{{\M@TitleReference {6.1}{Risultati dell'Ablation Study. Ogni riga rappresenta l'aggiunta di una componente rispetto alla precedente.\relax }}{32}{Risultati dell'Ablation Study. Ogni riga rappresenta l'aggiunta di una componente rispetto alla precedente.\relax }{table.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Dinamiche di Addestramento}{32}{subsection.6.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Dinamiche di apprendimento per le diverse configurazioni. Si noti come la configurazione finale (rossa) converga a un'accuratezza superiore nonostante una loss iniziale più alta dovuta alla difficoltà introdotta dall'augmentation.\relax }}{33}{figure.6.1}\protected@file@percent }
\newlabel{fig:accuracy_loss}{{\M@TitleReference {6.1}{Dinamiche di apprendimento per le diverse configurazioni. Si noti come la configurazione finale (rossa) converga a un'accuratezza superiore nonostante una loss iniziale più alta dovuta alla difficoltà introdotta dall'augmentation.\relax }}{33}{Dinamiche di apprendimento per le diverse configurazioni. Si noti come la configurazione finale (rossa) converga a un'accuratezza superiore nonostante una loss iniziale più alta dovuta alla difficoltà introdotta dall'augmentation.\relax }{figure.6.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Dinamiche reali dell'addestramento finale (200 epoche). I dati evidenziano la stabilità della convergenza grazie allo schedule Cosine Annealing.\relax }}{33}{figure.6.2}\protected@file@percent }
\newlabel{fig:real_dynamics}{{\M@TitleReference {6.2}{Dinamiche reali dell'addestramento finale (200 epoche). I dati evidenziano la stabilità della convergenza grazie allo schedule Cosine Annealing.\relax }}{33}{Dinamiche reali dell'addestramento finale (200 epoche). I dati evidenziano la stabilità della convergenza grazie allo schedule Cosine Annealing.\relax }{figure.6.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Analisi del Modello Finale (MobileNetECA-Rep)}{34}{section.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Matrice di Confusione}{34}{subsection.6.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Matrice di Confusione del modello MobileNetECA-Rep sul Test Set.\relax }}{34}{figure.6.3}\protected@file@percent }
\newlabel{fig:confusion_matrix}{{\M@TitleReference {6.3}{Matrice di Confusione del modello MobileNetECA-Rep sul Test Set.\relax }}{34}{Matrice di Confusione del modello MobileNetECA-Rep sul Test Set.\relax }{figure.6.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Curve ROC e AUC}{35}{subsection.6.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Dettaglio delle curve ROC per le 10 classi (Zoom TPR 0.8-1.0). L'area sotto la curva (AUC) è prossima a 1.0 per tutte le classi, indicando un'eccellente separabilità.\relax }}{35}{figure.6.4}\protected@file@percent }
\newlabel{fig:roc_zoomed}{{\M@TitleReference {6.4}{Dettaglio delle curve ROC per le 10 classi (Zoom TPR 0.8-1.0). L'area sotto la curva (AUC) è prossima a 1.0 per tutte le classi, indicando un'eccellente separabilità.\relax }}{35}{Dettaglio delle curve ROC per le 10 classi (Zoom TPR 0.8-1.0). L'area sotto la curva (AUC) è prossima a 1.0 per tutte le classi, indicando un'eccellente separabilità.\relax }{figure.6.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Analisi Qualitativa}{36}{section.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Esempi di Classificazione Corretta}{36}{subsection.6.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Analisi degli Errori - Misclassification}{36}{subsection.6.3.2}\protected@file@percent }
\citation{he2016deep}
\citation{he2016deep}
\citation{he2016deep}
\citation{he2016deep}
\citation{simonyan2014very}
\citation{sandler2018mobilenetv2}
\citation{sandler2018mobilenetv2}
\citation{ma2018shufflenet}
\citation{ding2021repvgg}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Esempi di errata classificazione (Test Set). Ogni riquadro mostra l'immagine originale con l'etichetta vera (True), quella predetta (Pred) e la confidenza del modello. Si noti come alcune immagini siano ambigue o difficili anche per un osservatore umano.\relax }}{37}{figure.6.5}\protected@file@percent }
\newlabel{fig:misclassified}{{\M@TitleReference {6.5}{Esempi di errata classificazione (Test Set). Ogni riquadro mostra l'immagine originale con l'etichetta vera (True), quella predetta (Pred) e la confidenza del modello. Si noti come alcune immagini siano ambigue o difficili anche per un osservatore umano.\relax }}{37}{Esempi di errata classificazione (Test Set). Ogni riquadro mostra l'immagine originale con l'etichetta vera (True), quella predetta (Pred) e la confidenza del modello. Si noti come alcune immagini siano ambigue o difficili anche per un osservatore umano.\relax }{figure.6.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Confronto con lo Stato dell'Arte: Efficienza}{37}{section.6.4}\protected@file@percent }
\newlabel{tab:comparison_sota}{{\M@TitleReference {\caption@xref {tab:comparison_sota}{ on input line 4}}{Confronto con lo Stato dell'Arte: Efficienza}}{38}{Confronto con lo Stato dell'Arte: Efficienza}{section.6.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces Confronto con lo Stato dell'Arte su CIFAR-10. Il nostro modello (MobileNetECA-Rep) ottiene risultati competitivi con una frazione dei parametri e dei FLOPs.\relax }}{38}{table.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}Discussione}{38}{subsection.6.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}Analisi Empirica della Latenza e Implementabilità}{38}{subsection.6.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Efficiency Frontier: Accuratezza vs Numero di Parametri (scala logaritmica). Il nostro modello (stella rossa) si trova nettamente a sinistra (meno parametri) rispetto a modelli con accuratezza simile come ResNet-32 o VGG-16.\relax }}{39}{figure.6.6}\protected@file@percent }
\newlabel{fig:efficiency}{{\M@TitleReference {6.6}{Efficiency Frontier: Accuratezza vs Numero di Parametri (scala logaritmica). Il nostro modello (stella rossa) si trova nettamente a sinistra (meno parametri) rispetto a modelli con accuratezza simile come ResNet-32 o VGG-16.\relax }}{39}{Efficiency Frontier: Accuratezza vs Numero di Parametri (scala logaritmica). Il nostro modello (stella rossa) si trova nettamente a sinistra (meno parametri) rispetto a modelli con accuratezza simile come ResNet-32 o VGG-16.\relax }{figure.6.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces Latenza di Inferenza media su CPU (1000 iterazioni)\relax }}{39}{table.6.3}\protected@file@percent }
\newlabel{tab:latency}{{\M@TitleReference {6.3}{Latenza di Inferenza media su CPU (1000 iterazioni)\relax }}{39}{Latenza di Inferenza media su CPU (1000 iterazioni)\relax }{table.6.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Analisi dell'Impatto degli Iperparametri}{40}{section.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.1}Interazione Learning Rate vs Weight Decay}{40}{subsection.6.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Heatmap dell'accuratezza (Validation Accuracy) al variare di Learning Rate e Weight Decay. Si osserva che alti valori di LR richiedono un decay maggiore per stabilizzare l'addestramento.\relax }}{40}{figure.6.7}\protected@file@percent }
\newlabel{fig:heatmap_lr_wd}{{\M@TitleReference {6.7}{Heatmap dell'accuratezza (Validation Accuracy) al variare di Learning Rate e Weight Decay. Si osserva che alti valori di LR richiedono un decay maggiore per stabilizzare l'addestramento.\relax }}{40}{Heatmap dell'accuratezza (Validation Accuracy) al variare di Learning Rate e Weight Decay. Si osserva che alti valori di LR richiedono un decay maggiore per stabilizzare l'addestramento.\relax }{figure.6.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.2}Impatto della Capacità del Modello (Width)}{41}{subsection.6.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces Accuratezza in funzione di Learning Rate e Width Multiplier. Reti più larghe (Width > 0.5) beneficiano di LR più alti, mentre reti molto sottili richiedono un approccio più conservativo.\relax }}{41}{figure.6.8}\protected@file@percent }
\newlabel{fig:heatmap_lr_width}{{\M@TitleReference {6.8}{Accuratezza in funzione di Learning Rate e Width Multiplier. Reti più larghe (Width > 0.5) beneficiano di LR più alti, mentre reti molto sottili richiedono un approccio più conservativo.\relax }}{41}{Accuratezza in funzione di Learning Rate e Width Multiplier. Reti più larghe (Width > 0.5) beneficiano di LR più alti, mentre reti molto sottili richiedono un approccio più conservativo.\relax }{figure.6.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces Interazione tra Width Multiplier e Weight Decay. Reti più piccole sono meno soggette a overfitting massiccio ma beneficiano comunque di una moderata regolarizzazione.\relax }}{42}{figure.6.9}\protected@file@percent }
\newlabel{fig:heatmap_width_wd}{{\M@TitleReference {6.9}{Interazione tra Width Multiplier e Weight Decay. Reti più piccole sono meno soggette a overfitting massiccio ma beneficiano comunque di una moderata regolarizzazione.\relax }}{42}{Interazione tra Width Multiplier e Weight Decay. Reti più piccole sono meno soggette a overfitting massiccio ma beneficiano comunque di una moderata regolarizzazione.\relax }{figure.6.9}{}}
\@input{appendice.aux}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{Conclusioni e Sviluppi Futuri}{44}{appendix*.4}\protected@file@percent }
\@writefile{loa}{\addvspace {10\p@ }}
\citation{*}
\bibdata{bibliografia}
\bibcite{krizhevsky2009learning}{1}
\bibcite{hendrycks2016gaussian}{2}
\bibcite{he2016deep}{3}
\bibcite{howard2017mobilenets}{4}
\bibcite{sandler2018mobilenetv2}{5}
\bibcite{hu2018squeeze}{6}
\bibcite{wang2020eca}{7}
\@writefile{toc}{\contentsline {chapter}{Bibliografia}{51}{appendix*.15}\protected@file@percent }
\bibcite{ding2021repvgg}{8}
\bibcite{simonyan2014very}{9}
\bibcite{ma2018shufflenet}{10}
\bibcite{goodfellow2016deep}{11}
\bibcite{lecun2015deep}{12}
\bibcite{cubuk2019autoaugment}{13}
\bibcite{zhong2020random}{14}
\memsetcounter{lastsheet}{62}
\memsetcounter{lastpage}{52}
\gdef \@abspage@last{62}
