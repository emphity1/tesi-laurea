\babel@toc {italian}{}\relax 
\babel@toc {italian}{}\relax 
\contentsline {chapter}{Introduzione}{iii}{chapter*.1}%
\contentsline {chapter}{Indice}{iv}{section*.2}%
\contentsline {chapter}{Elenco delle figure}{viii}{section*.3}%
\contentsline {chapter}{\chapternumberline {1}Introduzione}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Contesto e Motivazione}{1}{section.1.1}%
\contentsline {section}{\numberline {1.2}Obiettivi della Tesi}{2}{section.1.2}%
\contentsline {section}{\numberline {1.3}Contributi del Lavoro}{2}{section.1.3}%
\contentsline {section}{\numberline {1.4}Struttura della Tesi}{3}{section.1.4}%
\contentsline {chapter}{\chapternumberline {2}Il Dataset CIFAR-10}{5}{chapter.2}%
\contentsline {section}{\numberline {2.1}Descrizione del Dataset CIFAR-10}{5}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}Struttura e Organizzazione dei Dati}{6}{subsection.2.1.1}%
\contentsline {section}{\numberline {2.2}Preprocessing dei Dati}{7}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Normalizzazione}{7}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Data Augmentation Standard}{8}{subsection.2.2.2}%
\contentsline {section}{\numberline {2.3}Sfide e Limitazioni}{8}{section.2.3}%
\contentsline {chapter}{\chapternumberline {3}Fondamenti delle Reti Neurali}{10}{chapter.3}%
\contentsline {section}{\numberline {3.1}Apprendimento Automatico}{10}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}Paradigmi di Apprendimento}{10}{subsection.3.1.1}%
\contentsline {subsection}{\numberline {3.1.2}Il Perceptron e il Neurone Artificiale}{11}{subsection.3.1.2}%
\contentsline {section}{\numberline {3.2}Funzioni di Attivazione}{12}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Sigmoide e Tanh}{12}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}ReLU (Rectified Linear Unit)}{12}{subsection.3.2.2}%
\contentsline {subsection}{\numberline {3.2.3}GELU (Gaussian Error Linear Unit)}{13}{subsection.3.2.3}%
\contentsline {section}{\numberline {3.3}Addestramento della Rete}{13}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}Funzione di Costo (Loss Function)}{13}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2}Algoritmo di Backpropagation}{13}{subsection.3.3.2}%
\contentsline {section}{\numberline {3.4}Ottimizzazione e Regolarizzazione}{14}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1}Ottimizzatori}{14}{subsection.3.4.1}%
\contentsline {subsubsection}{\numberline {3.4.1.1}Stochastic Gradient Descent (SGD)}{14}{subsubsection.3.4.1.1}%
\contentsline {subsubsection}{\numberline {3.4.1.2}SGD con Momentum}{14}{subsubsection.3.4.1.2}%
\contentsline {subsubsection}{\numberline {3.4.1.3}Adam (Adaptive Moment Estimation)}{15}{subsubsection.3.4.1.3}%
\contentsline {subsection}{\numberline {3.4.2}Tecniche di Regolarizzazione}{15}{subsection.3.4.2}%
\contentsline {subsubsection}{\numberline {3.4.2.1}Batch Normalization}{15}{subsubsection.3.4.2.1}%
\contentsline {subsubsection}{\numberline {3.4.2.2}Dropout}{15}{subsubsection.3.4.2.2}%
\contentsline {subsubsection}{\numberline {3.4.2.3}Weight Decay (L2 Regularization)}{15}{subsubsection.3.4.2.3}%
\contentsline {chapter}{\chapternumberline {4}Stato dell'Arte e Tecniche di Efficienza}{16}{chapter.4}%
\contentsline {section}{\numberline {4.1}Evoluzione delle Architetture CNN}{16}{section.4.1}%
\contentsline {subsection}{\numberline {4.1.1}Dalle Origini al Deep Learning Moderno}{16}{subsection.4.1.1}%
\contentsline {section}{\numberline {4.2}Architetture Efficienti per Mobile}{17}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}Depthwise Separable Convolutions}{17}{subsection.4.2.1}%
\contentsline {subsection}{\numberline {4.2.2}MobileNetV2: Inverted Residuals}{17}{subsection.4.2.2}%
\contentsline {section}{\numberline {4.3}Meccanismi di Attenzione}{18}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Squeeze-and-Excitation (SE)}{18}{subsection.4.3.1}%
\contentsline {subsection}{\numberline {4.3.2}Efficient Channel Attention (ECA)}{19}{subsection.4.3.2}%
\contentsline {section}{\numberline {4.4}Structural Reparameterization (RepVGG)}{19}{section.4.4}%
\contentsline {subsection}{\numberline {4.4.1}Il Principio di Equivalenza}{19}{subsection.4.4.1}%
\contentsline {chapter}{\chapternumberline {5}Progettazione e Architettura del Sistema}{20}{chapter.5}%
\contentsline {section}{\numberline {5.1}Panoramica del Sistema}{20}{section.5.1}%
\contentsline {section}{\numberline {5.2}Structural Reparameterization}{21}{section.5.2}%
\contentsline {subsection}{\numberline {5.2.1}Concetto Chiave: Training vs Inference}{21}{subsection.5.2.1}%
\contentsline {subsection}{\numberline {5.2.2}Fusione Matematica dei Kernel: Derivazione Completa}{22}{subsection.5.2.2}%
\contentsline {section}{\numberline {5.3}Architettura Dettagliata}{23}{section.5.3}%
\contentsline {subsection}{\numberline {5.3.1}Inverted Residual Block con ECA}{23}{subsection.5.3.1}%
\contentsline {subsection}{\numberline {5.3.2}Scelte Architetturali Specifiche}{24}{subsection.5.3.2}%
\contentsline {subsubsection}{\numberline {5.3.2.1}Stride Iniziale}{24}{subsubsection.5.3.2.1}%
\contentsline {subsubsection}{\numberline {5.3.2.2}Attivazione: GELU vs ReLU}{24}{subsubsection.5.3.2.2}%
\contentsline {section}{\numberline {5.4}Implementazione del Deploy}{25}{section.5.4}%
\contentsline {subsection}{\numberline {5.4.1}ECA Block: Implementazione Dettagliata}{27}{subsection.5.4.1}%
\contentsline {subsubsection}{\numberline {5.4.1.1}Formula del Kernel Adattivo}{27}{subsubsection.5.4.1.1}%
\contentsline {subsubsection}{\numberline {5.4.1.2}Pipeline Operativa}{28}{subsubsection.5.4.1.2}%
\contentsline {section}{\numberline {5.5}Configurazione Sperimentale}{28}{section.5.5}%
\contentsline {subsection}{\numberline {5.5.1}Strategie di Data Augmentation Avanzate}{28}{subsection.5.5.1}%
\contentsline {subsection}{\numberline {5.5.2}Protocollo di Training}{29}{subsection.5.5.2}%
\contentsline {subsection}{\numberline {5.5.3}Learning Rate Schedule}{29}{subsection.5.5.3}%
\contentsline {section}{\numberline {5.6}Analisi della Complessità Teorica}{30}{section.5.6}%
\contentsline {chapter}{\chapternumberline {6}Analisi dei Risultati}{31}{chapter.6}%
\contentsline {section}{\numberline {6.1}Ablation Study: Impatto delle Componenti}{31}{section.6.1}%
\contentsline {subsection}{\numberline {6.1.1}Dinamiche di Addestramento}{32}{subsection.6.1.1}%
\contentsline {section}{\numberline {6.2}Analisi del Modello Finale (MobileNetECA-Rep)}{34}{section.6.2}%
\contentsline {subsection}{\numberline {6.2.1}Matrice di Confusione}{34}{subsection.6.2.1}%
\contentsline {subsection}{\numberline {6.2.2}Curve ROC e AUC}{35}{subsection.6.2.2}%
\contentsline {section}{\numberline {6.3}Analisi Qualitativa}{36}{section.6.3}%
\contentsline {subsection}{\numberline {6.3.1}Esempi di Classificazione Corretta}{36}{subsection.6.3.1}%
\contentsline {subsection}{\numberline {6.3.2}Analisi degli Errori - Misclassification}{36}{subsection.6.3.2}%
\contentsline {section}{\numberline {6.4}Confronto con lo Stato dell'Arte: Efficienza}{37}{section.6.4}%
\contentsline {subsection}{\numberline {6.4.1}Discussione}{38}{subsection.6.4.1}%
\contentsline {subsection}{\numberline {6.4.2}Analisi Empirica della Latenza e Implementabilità}{38}{subsection.6.4.2}%
\contentsline {section}{\numberline {6.5}Analisi dell'Impatto degli Iperparametri}{40}{section.6.5}%
\contentsline {subsection}{\numberline {6.5.1}Interazione Learning Rate vs Weight Decay}{40}{subsection.6.5.1}%
\contentsline {subsection}{\numberline {6.5.2}Impatto della Capacità del Modello (Width)}{41}{subsection.6.5.2}%
\contentsline {appendix}{\chapternumberline {A}Dettagli Metriche di Valutazione}{43}{appendix.Alph1}%
\contentsline {chapter}{Conclusioni e Sviluppi Futuri}{44}{appendix*.4}%
\contentsline {chapter}{Bibliografia}{51}{appendix*.15}%
